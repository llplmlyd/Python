# 用Python写网络爬虫  
##  第一个网络爬虫
  - 下载函数  
  - 容错 HTTPError URLError   
  - 用户代理 useragent，header  
  - 解析Robot.txt  
  - 相对链接转绝对链接  
  - 避免下载重复链接（网页互链的状态）
  - 避免爬虫陷阱（动态网页链接生成无穷尽）  
  - 下载限速delay 防止服务器过载  
